{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bittensor blockchain hosts multiple self-contained incentive mechanisms 'subnets'. Subnets are playing fields through which miners (those producing value) and validators (those producing consensus) determine together the proper distribution of TAO for the purpose of incentivizing the creation of value, i.e. generating digital commodities, such as intelligence, or data. Each consists of a wire protocol through which miners and validators interact and their method of interacting with Bittensor's chain consensus engine Yuma Consensus which is designed to drive these actors into agreement about who is creating value.\n",
    "\n",
    "[Website](https://bittensor.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitepaper Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bittensor project whitepaper proposes a market framework for machine intelligence, where intelligence systems rank each other's value and the rankings are recorded on a digital ledger to incentivize high-performing contributors. Key concepts discussed in the whitepaper include:\n",
    "\n",
    "- **Abstract Definition of Intelligence**: Bittensor starts by defining intelligence as a function trained to minimize loss over a dataset. The network is made up of peers, each holding network weight ('stake') represented on a digital ledger.\n",
    "\n",
    "- **Model**: It involves distributing stake to peers who minimize a loss-objective. Peers score each other via outputs used as inputs for ranking, setting weights on the ledger.\n",
    "\n",
    "- **Incentive Mechanism**: To prevent collusion, the incentive function limits rewards to peers achieving consensus. This assumes that no more than half of the stake is controlled by colluding peers, and rewards are distributed based on trustworthiness measured by the weights and stake.\n",
    "\n",
    "- **Consensus**: Peers reaching consensus are those trusted by over half the stake in the network. Consensus is determined by the digital ledger and affects the reward distribution.\n",
    "\n",
    "- **Bonds**: Bonds are introduced to incentivize correctly setting weights. They are analogous to securities in markets and bond a peer to the performance of others it ranks highly.\n",
    "\n",
    "- **Consensus Building**: To avoid collusion, the system increases the connections across the network until consensus is reached, ensuring the majority of rewards go to honest peers.\n",
    "\n",
    "- **Network Operation Steps**: The paper outlines the operational steps of a peer in the network, including broadcasting datasets, processing responses, learning weights, and updating the ranking and incentive structure.\n",
    "\n",
    "- **Tensor Standardization**: A common input/output format standardizes interactions between heterogeneous models and datasets, facilitating different intelligence systems working together.\n",
    "\n",
    "- **Conditional Computation**: To reduce bandwidth and selectively connect peers, the network adopts conditional computation, which dynamically combines outputs from selected peers.\n",
    "\n",
    "- **Knowledge Extraction**: Through distillation, network knowledge can be compressed into 'student' models to run offline, providing flexibility and redundancy.\n",
    "\n",
    "- **Learning Weights**: The importance of each peer is determined by a pruning score, correlating to the impact of its removal from the network.\n",
    "\n",
    "- **Collusion**: The paper discusses strategies to maintain dominance of honest peers in the network and ensure dishonest groups or 'cabals' do not gain disproportionate influence.\n",
    "\n",
    "- **Conclusion**: Bittensor presents an approach to decentralize and democratize machine intelligence production, measuring value collaboratively and rewarding contributions based on value created within the network.\n",
    "\n",
    "The whitepaper is a call for a high-resolution, collaborative benchmark that could offer a superior reward mechanism for machine intelligence, incentivizing knowledge production and availability in an open network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bittensor Stake Simulation\n",
    "\n",
    "The following algorithm simulates the dynamics of a system comprising two entities, each with an associated stake. It iteratively adjusts these stakes based on trust calculations and incentives over a predefined number of iterations. The simulation provides insights into the evolution of the entities' stakes relative to each other under the influence of the defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK: 99 STAKE_B: 0.00012063371993464691\n",
      "BLOCK: 99 STAKE_A: 0.9998793662800652\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tau = 0.1\n",
    "temp = 10\n",
    "stake_A = 0.51\n",
    "stake_B = 0.49\n",
    "history = []\n",
    "len_ = 100\n",
    "for block in range(len_):\n",
    "    total_stake = stake_A + stake_B\n",
    "    trust_A = 1/(1 + math.exp(-(stake_A/total_stake - 0.5) * temp))\n",
    "    trust_B = 1/(1 + math.exp(-(stake_B/total_stake - 0.5) * temp))\n",
    "    ranks_A = stake_A\n",
    "    ranks_B = stake_B\n",
    "    incentive_A = ranks_A * trust_A\n",
    "    incentive_B = ranks_B * trust_B\n",
    "    total_incentive = incentive_A + incentive_B\n",
    "    total_stake = stake_A + stake_B\n",
    "    stake_A += tau * total_stake * incentive_A / total_incentive\n",
    "    stake_B += tau * total_stake * incentive_B / total_incentive\n",
    "    if block == len_ - 1: print('BLOCK:', block, 'STAKE_B:', stake_B / (stake_A + stake_B))\n",
    "    if block == len_ - 1: print('BLOCK:', block, 'STAKE_A:', stake_A / (stake_A + stake_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Initialize the predefined stakes for entities `stake_A` and `stake_B` to a starting condition. Parameters `tau` and `temp` are set, which likely represent an adjustment factor and a temperature parameter, respectively. The temperature parameter is commonly used in algorithms involving probability and exploration of states, such as simulated annealing.\n",
    "\n",
    "### Trust Calculation\n",
    "At each iteration, the trust level for each entity is calculated using a sigmoid function. This function depends on the proportion of the stake that the entity holds compared to the total stake. The sigmoid function ensures a smooth transition from a minimum to a maximum value, suggesting a probabilistic interpretation of trust.\n",
    "\n",
    "### Rank and Incentive Computation\n",
    "The algorithm assigns a rank identical to the entity's stake and calculates the incentive as the product of the entity's rank and trust. This indicates that an entity's incentive is higher when it has both a larger stake and higher trust.\n",
    "\n",
    "### Stake Adjustment\n",
    "The stakes of both entities are updated according to their respective incentives, normalized by the total incentive. The `tau` parameter modulates the extent to which stakes are adjusted at each iteration.\n",
    "\n",
    "### Output\n",
    "The algorithm outputs the ratio of `stake_B` to the total stake at each iteration. This ratio is indicative of how `stake_B` changes in relation to the total stake in the system over time.\n",
    "\n",
    "### Conclusion\n",
    "By analyzing the output, we can infer the effects of initial conditions, parameter settings, and the algorithm's stability. After 100 iterations, the simulation results in `stake_B` holding a minuscule fraction of the total stake, demonstrating that `stake_A` becomes predominant. This could reflect long-term trends in resource allocation, reputation systems, or strategic adjustments in game-theoretical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `install.sh` file in the Bittensor repository is a shell script designed to install the Bittensor environment and dependencies. Here's what it does and how it works:\n",
    "\n",
    "1. **Determine OS**: It checks if the operating system is Linux or Darwin (macOS). If the system is neither, it will abort the installation.\n",
    "\n",
    "2. **Install Dependencies**:\n",
    "   - On Linux, it requires `apt` to install dependencies like `git`, `curl`, `cmake`, `build-essential`, `python3`, and `python3-pip`.\n",
    "   - On macOS, it uses Homebrew to install `xcode`, `git`, `cmake`, `python3`, and associated tools.\n",
    "\n",
    "3. **Install Bittensor**:\n",
    "   - For Linux, it clones the Bittensor repository into `~/.bittensor/bittensor`, then uses pip to install the package in editable mode.\n",
    "   - For macOS, the process is similar; cloning and pip installation steps are followed after ensuring Brew and CMake are installed.\n",
    "\n",
    "4. **Configuration Adjustments**:\n",
    "   - On Linux, it offers to increase `ulimit` to allow the miner to run longer without hitting file descriptor limits.\n",
    "\n",
    "5. **Final Messages**: It displays post-installation messages, including instructions on creating wallets, running a miner, and joining the Bittensor Discord community.\n",
    "\n",
    "To create a subnet using the `install.sh` script, the script itself doesn't directly handle subnet creation. However, after installing Bittensor using the script, you can create a subnet by running `btcli subnets create` as suggested in the `README.md` of the repository. The `btcli` command-line interface comes with functions to manage subnets, including listing and creating them.\n",
    "\n",
    "**Creating a Subnet**:\n",
    "After successful installation of Bittensor:\n",
    "\n",
    "```bash\n",
    "btcli subnets create\n",
    "```\n",
    "\n",
    "\n",
    "Other included `btcli` commands relevant to managing your Bittensor node:\n",
    "\n",
    "```bash\n",
    "# List subnets\n",
    "btcli subnets list\n",
    "\n",
    "# Create wallets\n",
    "btcli new_coldkey # For holding funds\n",
    "btcli new_hotkey  # For running miners\n",
    "\n",
    "# Run a miner\n",
    "python3 ~/.bittensor/bittensor/neurons/text/prompting/miners/gpt4all/neuron.py\n",
    "```\n",
    "\n",
    "\n",
    "Note that to actually create and manage subnets, you will primarily be interacting with the `btcli` command-line interface provided by Bittensor. The `install.sh` script's role is primarily for setting up the necessary environment and tools on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Bittensor wallet from Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "IMPORTANT: Store this mnemonic in a secure (preferable offline place), as anyone who has possession of this mnemonic can use it to regenerate the key and access your tokens. \n",
      "\u001b[0m\n",
      "The mnemonic to the new coldkey is:\n",
      "\n",
      "\u001b[32mmetal clay oyster clinic news verb jar spy way mass sausage trouble\u001b[0m\n",
      "\n",
      "You can use the mnemonic to recreate the key in case it gets lost. The command to use to regenerate the key using this mnemonic is:\n",
      "btcli w regen_coldkey --mnemonic metal clay oyster clinic news verb jar spy way mass sausage trouble\n",
      "\n",
      "Passwords do not match\n",
      "\u001b[31m\n",
      "IMPORTANT: Store this mnemonic in a secure (preferable offline place), as anyone who has possession of this mnemonic can use it to regenerate the key and access your tokens. \n",
      "\u001b[0m\n",
      "The mnemonic to the new hotkey is:\n",
      "\n",
      "\u001b[32mconnect vessel dumb right topple title tragic link burst audit inject pact\u001b[0m\n",
      "\n",
      "You can use the mnemonic to recreate the key in case it gets lost. The command to use to regenerate the key using this mnemonic is:\n",
      "btcli w regen_hotkey --mnemonic connect vessel dumb right topple title tragic link burst audit inject pact\n",
      "\n",
      "wallet(default, default, ~/.bittensor/wallets/)\n"
     ]
    }
   ],
   "source": [
    "import bittensor as bt\n",
    "\n",
    "# Initialize the wallet object\n",
    "wallet = bt.wallet()\n",
    "\n",
    "# Create a new coldkey\n",
    "wallet.create_new_coldkey()\n",
    "\n",
    "# Create a new hotkey\n",
    "wallet.create_new_hotkey()\n",
    "\n",
    "print(wallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info see [Bittensor Wallet](https://bittensor.com/documentation/getting-started/wallets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coldkey vs hotkey:\n",
    "\n",
    "- **Coldkey**: This is the main key for a Bittensor wallet. It is designed for secure storage and high-value operations. A coldkey is used for securely storing funds, executing transfers between addresses, and managing staking operations. Coldkeys can have many associated hotkeys but are not meant for frequent, daily operations.\n",
    "\n",
    "- **Hotkey**: A hotkey is used for online operations within the Bittensor network. This includes activities like signing queries, running miners, and performing validations. Unlike coldkeys, hotkeys are intended for regular use and are associated with operations that require a lesser security threshold. Each hotkey is linked to only one coldkey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yuma Consensus\n",
    "\n",
    "\"Yuma Consensus\" is related to the governance and development process for Bittensor, rather than a specific type of consensus algorithm like Proof of Work or Proof of Stake. It involves extensive discussion and peer review, particularly when changes to consensus-critical code are proposed. For such changes, discussions are held on various platforms like a discord server, and they often need to be accompanied by a BIP (Bittensor Improvement Proposal). Decisions are made based on the collective judgement of maintainers and require a widely perceived technical consensus that the change is beneficial for the wider community. The name \"Yuma Consensus\" could be derived from Yuma Rao, the individual likely behind its conceptual development or subject to copyright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid 123  owned by hotkey: 5HgKApmbGpjEeAFVAYGtrTmYTdugbdWcnG2jq1BLLvxkGjX4 assoicated with coldkey: 5DAchgvPkoexHJnPURimXdyz1VVU2q9HzeqzsFVHc9SADHoj\n"
     ]
    }
   ],
   "source": [
    "# Subnetwork are composed of a discrete number of \"uids\" under which validators and miners position themselves and over which Yuma Consensus is run.\n",
    "subnet = bt.metagraph(netuid = 1)\n",
    "uid = 123\n",
    "print('uid', uid, ' owned by hotkey:', subnet.hotkeys[uid], 'assoicated with coldkey:', subnet.coldkeys[uid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subnet\n",
    "\n",
    "The token based mechanism under which the miners are incentivized ensures that they are constantly driven to make their knowledge output more useful, in terms of speed, intelligence and diversity. The value generated by the network is distributed directly to the individuals producing that value, without intermediaries. Anyone can participate in this endeavour, extract value from the network, and govern Bittensor. The network is open to all participants, and no individual or group has full control over what is learned, who can profit from it, or who can access it.\n",
    "\n",
    "Project maintainers reserve the right to weigh the opinions of peer reviewers using common sense judgement and may also weigh based on merit. Reviewers that have demonstrated a deeper commitment and understanding of the project over time or who have clear domain expertise may naturally have more weight, as one would expect in all walks of life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bittensor/README.md\n",
    "  \n",
    "  The token based mechanism under which the miners are incentivized ensures that they are constantly driven to make their knowledge output more useful, in terms of speed, intelligence and diversity. The value generated by the network is distributed directly to the individuals producing that value, without intermediaries. Anyone can participate in this endeavour, extract value from the network, and govern Bittensor. The network is open to all participants, and no individual or group has full control over what is learned, who can profit from it, or who can access it.\n",
    "\n",
    "  Bittensor is a mining network, similar to Bitcoin, that includes built-in incentives designed to encourage computers to provide access to machine learning models in an efficient and censorship-resistant manner. These models can be queried by users seeking outputs from the network, for instance; generating text, audio, and images, or for extracting numerical representations of these input types. Under the hood, BittensorРђЎs *economic market*, is facilitated by a blockchain token mechanism, through which producers (***miners***) and the verification of the work done by those miners (***validators***) are rewarded. Miners host, train or otherwise procure machine learning systems into the network as a means of fulfilling the verification problems defined by the validators, like the ability to generate responses from prompts i.e. РђюWhat is the capital of Texas?.\n",
    "\n",
    "  #### Managing Subnets\n",
    "  ```bash\n",
    "  btcli subnets list\n",
    "  btcli subnets create\n",
    "  ```\n",
    "\n",
    "* bittensor/bittensor/mock/subtensor_mock.py\n",
    "```python\n",
    "info = SubnetInfo(\n",
    "    netuid=netuid,\n",
    "    rho=query_subnet_info(name=\"Rho\"),\n",
    "    kappa=query_subnet_info(name=\"Kappa\"),\n",
    "    difficulty=query_subnet_info(name=\"Difficulty\"),\n",
    "    immunity_period=query_subnet_info(name=\"ImmunityPeriod\"),\n",
    "    max_allowed_validators=query_subnet_info(name=\"MaxAllowedValidators\"),\n",
    "    min_allowed_weights=query_subnet_info(name=\"MinAllowedWeights\"),\n",
    "    max_weight_limit=query_subnet_info(name=\"MaxWeightLimit\"),\n",
    "    scaling_law_power=query_subnet_info(name=\"ScalingLawPower\"),\n",
    "    subnetwork_n=query_subnet_info(name=\"SubnetworkN\"),\n",
    "    max_n=query_subnet_info(name=\"MaxAllowedUids\"),\n",
    "    blocks_since_epoch=query_subnet_info(name=\"BlocksSinceLastStep\"),\n",
    "    tempo=query_subnet_info(name=\"Tempo\"),\n",
    "    modality=query_subnet_info(name=\"NetworkModality\"),\n",
    "    connection_requirements={\n",
    "        str(netuid_.value): percentile.value\n",
    "        for netuid_, percentile in self.query_map_subtensor(\n",
    "            name=\"NetworkConnect\", block=block, params=[netuid]\n",
    "        ).records\n",
    "    },\n",
    "    emission_value=query_subnet_info(name=\"EmissionValues\"),\n",
    "    burn=query_subnet_info(name=\"Burn\"),\n",
    "    owner_ss58=query_subnet_info(name=\"SubnetOwner\"),\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SubnetInfo` class has the following parameters, each corresponding to a specific configuration or status information about the subnet:\n",
    "\n",
    "1. `netuid`: A unique identifier for the subnet.\n",
    "2. `rho`: Parameter representing the rewards multiplier, which can adjust the incentive model for validators.\n",
    "3. `kappa`: Staking requirement multiplier or a related security parameter.\n",
    "4. `difficulty`: Represents the computational difficulty for validating or mining on this subnet.\n",
    "5. `immunity_period`: A time frame during which new validators might be exempt from certain network penalizations to facilitate onboarding.\n",
    "6. `max_allowed_validators`: The maximum number of validators that are permitted on the subnet.\n",
    "7. `min_allowed_weights`: The minimum total weight that is required to be considered a validator on the subnet.\n",
    "8. `max_weight_limit`: The absolute maximum weight that a validator may accumulate.\n",
    "9. `scaling_law_power`: A parameter involved in the calculation of rewards or validator influence that follows a specific scaling law.\n",
    "10. `subnetwork_n`: The number of subnetworks or the index of the current subnetwork.\n",
    "11. `max_n`: The maximum number of unique identifiers allowed on the subnet.\n",
    "12. `blocks_since_epoch`: The number of blocks created since the last epoch step or a significant reset point.\n",
    "13. `tempo`: It likely refers to the pace or rate at which the subnet operates, which could involve block times or reward distribution intervals.\n",
    "14. `modality`: The mode of operation or the type of consensus or validation utilized by the subnet.\n",
    "15. `connection_requirements`: A dictionary mapping between subnet unique identifiers and their connection percentile values, which might define the requirements for nodes to establish connections within the subnet.\n",
    "16. `emission_val`: Could represent the number or value of tokens being released as rewards on the subnet. This parameter is incomplete in the given context.\n",
    "\n",
    "These parameters collectively define the operating conditions, security features, and economic model of the subnet within the Bittensor network. Each parameter might influence how the network operates and how validators interact with the network and each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_do_setup_subnet` function in `test_subtensor_integration.py` does the following:\n",
    "\n",
    "* bittensor/tests/integration_tests/test_subtensor_integration.py\n",
    "  ```python\n",
    "  @classmethod\n",
    "  def _do_setup_subnet(cls):\n",
    "      # reset the mock subtensor\n",
    "      cls._mock_subtensor.reset()\n",
    "      # Setup the mock subnet 3\n",
    "      cls._mock_subtensor.create_subnet(netuid=3)\n",
    "  ```\n",
    "\n",
    "1. **Reset Mock Subtensor**: The mock subtensor instance is reset to a clean state to ensure the test starts without any prior state that might affect the results. This is achieved using the `cls._mock_subtensor.reset()` method call.\n",
    "\n",
    "2. **Create Subnet**: A new mock subnet with a specified `netuid` is created to simulate the presence of a subnet in the network. This helps in testing the registration and other subnet-related operations. In the example provided, `cls._mock_subtensor.create_subnet(netuid=3)` creates a subnet with `netuid` set to 3.\n",
    "\n",
    "The second and third mentions of `_do_setup_subnet` in the payload appear to be related to the mocking of other subtensor-related methods, such as setting the difficulty level for the subnet and simulating the process of registration of neurons with the mocked `_do_pow_register` method.\n",
    "\n",
    "Usage details of `_do_setup_subnet` in integration tests:\n",
    "- The function is typically used at the beginning of integration tests to configure the test environment with necessary preconditions such as a working subnet.\n",
    "- It allows developers to focus on testing the actual behavior of the Bittensor protocol without worrying about setting up a real or fully functional subnet.\n",
    "- The function is usually called without parameters as it is designed to configure a standard test environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and use a new mock subnet for integration testing in Bittensor's testing environment, you can follow these steps derived from the information provided in `test_subtensor_integration.py`.\n",
    "\n",
    "Here's a detailed description and example code on how to create and use a mock subnet:\n",
    "\n",
    "1. **Resetting Mock Subtensor**: Before creating a new subnet, it is a good practice to reset the state of the existing mock subtensor. This ensures that previous test states do not affect the current test setup.\n",
    "\n",
    "\n",
    "```python\n",
    "cls._mock_subtensor.reset()\n",
    "```\n",
    "\n",
    "\n",
    "2. **Creating a Mock Subnet**:\n",
    "To create a mock subnet you can call the `create_subnet` method on the instance of `MockSubtensor`, passing it the unique network identifier (`netuid`).\n",
    "\n",
    "\n",
    "```python\n",
    "cls._mock_subtensor.create_subnet(netuid=3)\n",
    "```\n",
    "\n",
    "\n",
    "**How to Use**:\n",
    "\n",
    "The `_do_setup_subnet` function allows each test to operate under the assumption that a new subnet is already created and ready to be interacted with.\n",
    "\n",
    "Here's an example test that might work with the mock subnet:\n",
    "\n",
    "\n",
    "```python\n",
    "def test_subnet_interaction(self):\n",
    "    # This test can now interact with the mock subnet that was set up.\n",
    "    mock_neuron = self._mock_subtensor.get_neuron_for_pubkey_and_subnet(pubkey='some_pubkey', netuid=3)\n",
    "    # Perform assertions or other test commands using mock_neuron.\n",
    "```\n",
    "\n",
    "\n",
    "When writing your actual integration tests, you will follow the structure of the test class, utilizing `setUpClass`, `tearDownClass`, and individual test methods to interact with the mock network and assert certain behaviors or properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metagraph\n",
    "\n",
    "The neural network metagraph in Bittensor functions as a decentralized data structure that holds information about the different machine learning models (neurons) participating in the network. It tracks the performance and trust of these neurons based on the validation work done by validators.\n",
    "\n",
    "Validators work with the metagraph by performing verification tasks and returning `torch.FloatTensor` trust values. These trust values represent the validators' assessment of the neuron's performance or reliability. This information is then used by the metagraph to update the network's understanding of each neuron's value and may influence how often a neuron is queried or rewarded.\n",
    "\n",
    "Here is a direct reference to the return value from the metagraph's validator trust:\n",
    "\n",
    "\n",
    "```python\n",
    "# metagraph.py\n",
    "def validator_trust(self):\n",
    "    return self.validator_trust\n",
    "```\n",
    "\n",
    "\n",
    "The validators’ role is crucial for maintaining the integrity of the Bittensor network and for the token-based incentive mechanism that rewards miners. Validators help to ensure that only honest and efficient machine learning models are incentivized which, in turn, fosters a healthier and more useful network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Validator\n",
    "\n",
    "Validators in the Bittensor network play a role in maintaining the integrity and security of the peer-to-peer network. Validators are responsible for verifying transactions and potentially creating new blocks within the network's blockchain. While the specific details of a validator's role within the Bittensor network were not included in the provided matches, typically, validators also participate in consensus mechanisms, help to govern the network, and are incentivized through a token-based mechanism. This ensures validators are rewarded for their efforts in providing useful computations to the network, maintaining its smooth operation, and preventing malicious activities.\n",
    "\n",
    "```bash\n",
    "# List all subnets to find the one you are interested in\n",
    "btcli subnets list\n",
    "\n",
    "# If the subnet does not exist, create it (requires permissions)\n",
    "btcli subnets create --subnetwork {desired_netuid}\n",
    "\n",
    "# Register as a validator on the subnet\n",
    "btcli register --netuid {target_subnet_netuid}\n",
    "```\n",
    "\n",
    "\n",
    "Replace `{desired_netuid}` with your specified netuid for the new subnet if you are creating one, and replace `{target_subnet_netuid}` with the netuid of the subnet you wish to validate.\n",
    "\n",
    "Please ensure you have the required permissions and balances to perform these operations on the Bittensor network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a custom class validator to verify subnet integrity, you need to follow a few steps, taking inspiration from the provided matches in Bittensor's codebase:\n",
    "\n",
    "```python\n",
    "class SubnetValidator:\n",
    "    def __init__(self, subtensor, netuid):\n",
    "        self.subtensor = subtensor\n",
    "        self.netuid = netuid\n",
    "\n",
    "    def verify_subnet_exists(self):\n",
    "        if not self.subtensor.subnet_exists(netuid=self.netuid):\n",
    "            bittensor.__console__.print(f\"[red]Subnet {self.netuid} does not exist[/red]\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Implement additional verification methods as needed\n",
    "```\n",
    "\n",
    "1. **Integrate with Subtensor**:\n",
    "   - Your custom validator depends on a `subtensor` object that has methods such as `subnet_exists`. You must integrate your validator with such an object to check against the actual network state.\n",
    "\n",
    "2. **Usage Example**:\n",
    "    - After defining your validator class, you can use it as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "subtensor = ... # Initialize your subtensor object here\n",
    "netuid = ...    # The netuid of the subnet you want to verify\n",
    "\n",
    "# Create an instance of the custom validator\n",
    "validator = SubnetValidator(subtensor, netuid)\n",
    "\n",
    "# Perform the actual verification\n",
    "if validator.verify_subnet_exists():\n",
    "    print(\"Subnet integrity verified.\")\n",
    "else:\n",
    "    print(\"Subnet integrity check failed.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validator Trust in Metagraph\n",
    "\n",
    "\n",
    "To implement validator trust within the metagraph and return a `torch.FloatTensor` in Bittensor, follow the example based on the snippets provided. Here's a hypothetical implementation that assumes you have a `Metagraph` class with a `validator_trust` property.\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class Metagraph:\n",
    "    # ... other properties and methods ...\n",
    "\n",
    "    @property\n",
    "    def validator_trust(self) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Validator trust values of the neurons.\n",
    "        \"\"\"\n",
    "        # This method should compute or retrieve the validator trust values.\n",
    "        # Below, we return a placeholder tensor for example purposes.\n",
    "        return torch.tensor([0.9, 0.1, 0.5])\n",
    "\n",
    "# Usage:\n",
    "metagraph = Metagraph()\n",
    "validator_trust_scores = metagraph.validator_trust\n",
    "print(validator_trust_scores)  # Outputs: tensor([0.9, 0.1, 0.5])\n",
    "```\n",
    "\n",
    "\n",
    "In the real implementation, `validator_trust` would contain logic to compute or retrieve the validators' trust scores, likely based on their performance, responsiveness, and history within the Bittensor network. The method should return a `torch.FloatTensor`, which is the format Bittensor uses for these scores.\n",
    "\n",
    "Remember, the actual computation behind the `validator_trust` property would be far more complex, involving blockchain interaction and perhaps AI model evaluation. The example placeholder tensor `[0.9, 0.1, 0.5]` represents the trust of each validator, with values typically ranging between 0 and 1, where higher values represent greater trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute a trust score for a subnet in Bittensor:\n",
    "\n",
    "1. Determine the relevant performance metrics for miners within the subnet. This includes their ability to provide useful knowledge output such as speed, intelligence, and diversity.\n",
    "\n",
    "2. Evaluate each miner based on these metrics. Validators within the network verify the work done by miners.\n",
    "\n",
    "3. Assign trust scores to each miner based on the evaluation. These scores may be normalized to fit within a certain range, like 0 to 1.\n",
    "\n",
    "4. Aggregate these individual miner scores to compute an overall subnet trust score. This could be an average, median, or other statistical measures based on individual trust scores.\n",
    "\n",
    "The network incentivizes miners by rewarding them for their contributions. This ensures that miners are motivated to improve their services, in turn contributing to the overall trustworthiness of the subnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Validators\n",
    "\n",
    "The OpenValidators repository contains Bittensor Validators designed by the OpenTensor Foundation for the community. Its main goal is to facilitate interaction with the Bittensor network by providing open-source validators. These validators query the network for responses and evaluations, which are then assessed by a pipeline of reward functions including diversity, relevance, rlhf, among others. The validators are also designed for TAO holders who wish to build or run validators developed by the foundation. There are four main avenues for engaging with the repository as mentioned in the usage section of the README, though the full details on these avenues are not provided in the extracted text.\n",
    "\n",
    "[Repository](https://github.com/opentensor/validators)\n",
    "\n",
    "To install OpenValidators, you can follow these steps from the repository's README:\n",
    "\n",
    "From source:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/opentensor/validators.git\n",
    "pip3 install -e openvalidators/\n",
    "```\n",
    "\n",
    "\n",
    "Alternatively, to install openvalidators from the root folder:\n",
    "\n",
    "```bash\n",
    "pip install -e ../setup.py\n",
    "```\n",
    "\n",
    "\n",
    "Additionally, you should:\n",
    "1. Install Weights and Biases by following the guide at https://docs.wandb.ai/quickstart and then run `wandb login` within the repository.\n",
    "2. Install PM2 and the `jq` package:\n",
    "\n",
    "   **On Linux**:\n",
    "   \n",
    "```bash\n",
    "   sudo apt update && sudo apt install jq && sudo apt install npm && sudo npm install pm2 -g && pm2 update\n",
    "   ```\n",
    "\n",
    "\n",
    "   **On Mac OS**:\n",
    "   \n",
    "```bash\n",
    "   brew update && brew install jq && brew install npm && sudo npm install pm2 -g && pm2 update\n",
    "   ```\n",
    "\n",
    "3. Run the `run.sh` script to handle running your validator and pulling the latest updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Participation in Network Validation is available to TAO holders. The validation mechanism utilizes a dual proof-of-stake and proof-of-work system known as Yuma Consensus, which you can learn more about here. To start validating, you will need to have a Bittensor wallet with a sufficient amount of TAO tokens staked.\n",
    "\n",
    "Once you have your wallet ready for validation, you can start the foundation validator by running the following command:\n",
    "\n",
    "```bash\n",
    "python3 validators/openvalidators/neuron.py --wallet.name <your-wallet-name> --wallet.hotkey <your-wallet-hot-key>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `neuron.py` class initializes a validator and registers it to the metagraph as follows:\n",
    "\n",
    "1. **Initialization**: \n",
    "    - It starts by setting up the necessary configurations by calling `neuron.config()`.\n",
    "    - It then checks these configurations using `self.check_config(self.config)`.\n",
    "    - Logging is initialized with the given configurations and a specified logging directory.\n",
    "    - The configuration details are printed to the console, and an information log statement is made indicating the initiation of the `neuron` class.\n",
    "\n",
    "2. **Wallet Creation**:\n",
    "    - The script initializes the wallet by calling `self.wallet = bt.wallet(config=self.config)`. \n",
    "    - If the wallet does not already exist, it is created with `self.wallet.create_if_non_existent()`.\n",
    "    - It verifies that the wallet is registered on the subtensor network by checking the hotkey registration using `self.subtensor.is_hotkey_registered_on_subnet`. If the wallet's hotkey is not registered, an exception is raised.\n",
    "\n",
    "3. **Metagraph Registration**:\n",
    "    - The metagraph object is initialized with the network UID and the Subtensor network specified in the configuration, with synchronization initially set to `False`. This is done to prevent unintended syncing without the subtensor object present.\n",
    "    - The metagraph is then synchronized with the subtensor to update its internal state according to the live Subtensor blockchain data, using `self.metagraph.sync(subtensor=self.subtensor)`.\n",
    "\n",
    "Through these operations, `neuron.py` sets up a validator that is capable of interacting with the Bittensor network, has a wallet set up to potentially transact within the network, and a synchronized metagraph that is aware of the state of the network. This allows the validator to participate properly in the Bittensor ecosystem, both for transactional purposes and for participating in the consensus mechanisms of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subnet 1: text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section looks at the `text-prompting` project in the Bittensor [Repository](https://github.com/opentensor/text-prompting). This project contains code for running a text-prompting miner, which is a miner that generates text responses to prompts. The miner is based on the Llama2 and similar models. Rewards are defined by sub-classing the `BaseRewardModel` class and implementing the `get_rewards` method. One then can combine multiple rewards to a common reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining the BaseRewardModel Class on the Example of the DirectPreferenceRewardModel\n",
    "\n",
    "In the OpenValidators context, rewards are computed using various models that evaluate aspects like relevance and diversity of responses. The relevance model computes a reward based on how relevant the completion is to the input prompt. This might involve calculating the difference between the prompt and completion representations, possibly using functions like cosine similarity.\n",
    "\n",
    "Diversity rewards may involve analyzing historical rewards or embeddings to ensure that the responses are not repetitive and provide varied information. Other models, such as the ReciprocateRewardModel, may include different reward calculation methods aimed at aligning the validators’ behavior with the desired outcomes.\n",
    "\n",
    "Weights for new and old rewards are computed by considering the respective counts of past and new inputs, forming a blend of historical performance and new information. This ensures a balanced updating mechanism for the reward values.\n",
    "\n",
    "Overall, reward computation involves complex interactions between the input, the generated responses, and the pre-trained models which are leveraged to evaluate the quality and utility of these responses in the context of the Bittensor network.\n",
    "\n",
    "The following example showcases the `BaseRewardModel` implementation based on the DPO (Direct Preference Optimization) state-of-the-art paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class DirectPreferenceRewardModel(BaseRewardModel):\n",
    "    # The DirectPreferenceRewardModel is a subclass of BaseRewardModel. \n",
    "    # This suggests the model has specific functionalities for calculating rewards \n",
    "    # related to direct preferences as part of a reinforcement learning process.\n",
    "    reward_model_name: str = \"cerebras/btlm-3b-8k-base\"\n",
    "    # This class attribute specifies the path to the pre-trained language model \n",
    "    # that will be used for calculating the preference rewards.\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str: return RewardModelType.dpo.value\n",
    "    # A property method that provides the name of this reward model type.\n",
    " \n",
    "    def __init__(self, device: str):\n",
    "        # The initialization method for the class with `device` indicating where\n",
    "        # the computations will take place (e.g., 'cuda' for GPU or 'cpu').\n",
    "        super().__init__()\n",
    "        # Initializes the superclass BaseRewardModel.\n",
    "        self.device = device\n",
    "        # Stores the computation device.\n",
    "        self.penalty = 1.2\n",
    "        # A penalty parameter for repetition (hardcoded as per the referenced paper).\n",
    "        \n",
    "        # Initializes the tokenizer and model with the specified pre-trained model name.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(DirectPreferenceRewardModel.reward_model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(DirectPreferenceRewardModel.reward_model_name,\n",
    "                                                          trust_remote_code=True,\n",
    "                                                          torch_dtype=torch.float16).to(self.device)\n",
    "        # The above two lines instantiate the tokenizer and the language model and move them to the specified device.\n",
    "\n",
    "    def reward_single(self, prompt: str, completion: str, name: str, with_penalty=True) -> float:\n",
    "        # A method that calculates the reward for a single completion given a prompt.\n",
    "        with torch.no_grad():\n",
    "            # Temporarily disables gradient calculation to save memory and computations,\n",
    "            # since backward pass is not needed for inference.\n",
    "\n",
    "            if completion.strip() == '' or len(completion) <= 5:\n",
    "                return -11\n",
    "            \n",
    "            combined = self.tokenizer(prompt + completion, return_tensors=\"pt\").input_ids[0].to(self.device)\n",
    "            prompt_part = self.tokenizer(prompt, return_tensors=\"pt\").input_ids[0].to(self.device)\n",
    "            # Tokenizes the prompt and the combination of prompt and completion.\n",
    "\n",
    "            if self.tokenizer.model_max_length <= len(prompt_part):\n",
    "                return -11\n",
    "\n",
    "            if self.tokenizer.model_max_length < len(combined):\n",
    "                combined = combined[:self.tokenizer.model_max_length]\n",
    "            # Handles cases where the completion or combined input is too long for the model.\n",
    "\n",
    "            labels = combined.clone()\n",
    "            labels[:len(prompt_part)] = -100\n",
    "            labels = labels[1:]\n",
    "            loss_mask = (labels != -100)\n",
    "            labels[labels == -100] = 0\n",
    "            labels = labels.unsqueeze(0).unsqueeze(2)\n",
    "            # Prepares the labels tensor used for calculating the reward, masking out the prompt part.\n",
    "\n",
    "            logits = self.model(combined.unsqueeze(0)).logits\n",
    "            logits = logits[:, :-1, :]\n",
    "            # Retrieves the logit predictions from the model for each token in the sequence.\n",
    "\n",
    "            if with_penalty:\n",
    "                for i in range(len(prompt_part)+1, len(combined)-1):\n",
    "                    logit = logits[:,i,:].clone()\n",
    "                    inputs = combined[len(prompt_part):i].clone()\n",
    "                    logits[:,i,:] = self.logit_penalty(input_ids=inputs, logit=logit)\n",
    "            # Applies a penalty to reduce repetition if enabled.\n",
    "\n",
    "            logits = logits.log_softmax(-1)\n",
    "            # Applies a log softmax function to the logits to get log probabilities.\n",
    "\n",
    "            per_token_logps = torch.gather(logits, dim=2, index=labels).squeeze(2)\n",
    "            reward = (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n",
    "            reward = reward[0].cpu().detach()\n",
    "            # Calculates the average log probability for the completion tokens as the reward.\n",
    "\n",
    "            if torch.isnan(reward) or torch.isinf(reward):\n",
    "                return -11\n",
    "            return reward.item()\n",
    "            # Checks for NaN or infinite values in the reward, and returns the reward as a floating point number.\n",
    "\n",
    "    def get_rewards(self, prompt: str, completions: List[str], name: str) -> torch.FloatTensor:\n",
    "        # A method to calculate the reward for a list of completions given a single prompt.\n",
    "        rewards = torch.tensor([self.reward_single(prompt, completion, name) for completion in completions],\n",
    "                               dtype=torch.float32).to(self.device)\n",
    "        bt.logging.trace(f\"DirectPreferenceRewardModel | rewards: {rewards.tolist()}\")\n",
    "        return rewards\n",
    "        # Collects rewards for each completion into a tensor and logs the output.\n",
    "\n",
    "    def logit_penalty(self, input_ids: torch.LongTensor, logit: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # A method to apply a penalty to logits based on repetition.\n",
    "        uniques, counts = input_ids.unique(return_counts=True)\n",
    "        score = torch.gather(logit, 1, uniques.unsqueeze(0))\n",
    "\n",
    "        score = torch.where(score < 0, score * (self.penalty**counts), score / (self.penalty**counts))\n",
    "        logit.scatter_(1, uniques.unsqueeze(0), score.to(logit.dtype))\n",
    "        return logit\n",
    "        # Modifies logits to penalize repeated tokens, promoting diversity in the generated text.\n",
    "```\n",
    "\n",
    "The `DirectPreferenceRewardModel` class is designed to compute rewards for generated completions in a reinforcement learning setup by using the average log-probability of the language model's predictions. It also includes a method to penalize repetitions to encourage diverse text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DirectPreferenceRewardModel.reward_single` function computes a reward for a text completion given a prompt based on the direct preference optimization (DPO) approach using a pre-trained language model. Here are the components and their meanings:\n",
    "\n",
    "1. **Input Verification**: Check if the `completion` is empty or less than 5 characters. If so, return a low reward score (-11), indicating an almost zero probability, which means the completion is not useful.\n",
    "\n",
    "2. **Tokenization**: Tokenize the combined `prompt + completion` and just the `prompt` separately. This is necessary for understanding where the prompt ends and the completion begins in terms of token sequence lengths.\n",
    "\n",
    "3. **Sequence Length Verification**: If the prompt already exceeds the model's maximum sequence length, return a low reward score (-11), as the model can't process such a long prompt.\n",
    "\n",
    "4. **Truncation**: Ensure the tokenized sequence (combined prompt and completion) does not exceed the model's maximum sequence length by truncation if necessary.\n",
    "\n",
    "5. **Masking Prompt**: Create a `labels` tensor that only includes the completion's tokens (ignores the prompt tokens by masking them with `-100`).\n",
    "\n",
    "6. **Model Forward Pass**: Using the pre-trained language model, perform a forward pass to generate logits for the sequence tokens.\n",
    "\n",
    "7. **Optional Penalty**: If `with_penalty` is `True`, iterate over completion tokens and apply the `logit_penalty` function to adjust the logits. This encourages diverse and less repetitive text.\n",
    "\n",
    "8. **Log Probabilities**: Convert logits to log probabilities via a log softmax operation.\n",
    "\n",
    "9. **Gathering Log Probabilities**: Use the `torch.gather` operation to extract the log probabilities that correspond to the actual tokens of the completion.\n",
    "\n",
    "10. **Reward Calculation**: Compute the reward as the average of the gathered log probabilities for the completion tokens. This captures how likely, according to the language model, the produced completion is, given the prompt.\n",
    "\n",
    "11. **Penalty for Repetition**: The `logit_penalty` function adjusts the logits to penalize repeated tokens within generated completions, promoting diversity.\n",
    "\n",
    "12. **Handling NaN/Infinite Results**: Check for and handle `NaN` or infinite results by returning a low reward score (-11), ensuring that the reward always makes sense numerically.\n",
    "\n",
    "13. **Return Reward**: The computed reward is returned as a single floating-point number representing the model's average log probability for the completion's tokens given the prompt.\n",
    "\n",
    "The reward indicates the preference of one completion over others based on the language model's perception of the completion's naturalness or likelihood. A higher reward suggests a more natural or likely completion, while a lower (or negative) reward indicates an unnatural or unlikely completion. This system thus enables automated and quantitative evaluation of text completions for reinforcement learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the subnet used one can combine multiple state-of-the-art or ad hoc crafted rewards to a common reward function. For example, the [Bittensor/text-prompting](https://github.com/opentensor/text-prompting) subnet repository combines the following rewards in the `text-prompting/neurons/validators/validator.py` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```python\n",
    "    # Ensure reward function weights sum to 1.\n",
    "    if self.reward_weights.sum() != 1:\n",
    "        message = (\n",
    "            f\"Reward function weights do not sum to 1 (Current sum: {self.reward_weights.sum()}.)\"\n",
    "            f\"Check your reward config file at `reward/config.py` or ensure that all your cli reward flags sum to 1.\"\n",
    "        )\n",
    "        bt.logging.error(message)\n",
    "        raise Exception(message)\n",
    "\n",
    "    self.reward_functions = [\n",
    "        DirectPreferenceRewardModel(device=self.device)\n",
    "        if self.config.reward.dpo_weight > 0\n",
    "        else MockRewardModel(RewardModelType.dpo.value),\n",
    "        OpenAssistantRewardModel(device=self.device)\n",
    "        if self.config.reward.rlhf_weight > 0\n",
    "        else MockRewardModel(RewardModelType.rlhf.value),\n",
    "        ReciprocateRewardModel(device=self.device)\n",
    "        if self.config.reward.reciprocate_weight > 0\n",
    "        else MockRewardModel(RewardModelType.reciprocate.value),\n",
    "        DahoasRewardModel(path=self.config.neuron.full_path, device=self.device)\n",
    "        if self.config.reward.dahoas_weight > 0\n",
    "        else MockRewardModel(RewardModelType.dahoas.value),\n",
    "        PromptRewardModel(device=self.device)\n",
    "        if self.config.reward.prompt_based_weight > 0\n",
    "        else MockRewardModel(RewardModelType.prompt.value),\n",
    "    ]\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation of each part:\n",
    "\n",
    "- `DirectPreferenceRewardModel`: Provides rewards based on direct preference assessments, where preferences are explicitly based on the completion [Paper](https://arxiv.org/abs/2305.18290).\n",
    "- `OpenAssistantRewardModel`: Could be designed to generate rewards in the context of RLHF [Paper](https://arxiv.org/abs/2305.18438) and the open-source OpenAssistant collected data [Paper](https://arxiv.org/abs/2304.07327).\n",
    "- `ReciprocateRewardModel`: May encourage behaviors that are reciprocal in nature, reinforcing actions that elicit mutual benefits in interactions [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0960077918304405).\n",
    "- `DahoasRewardModel`: DAHOAS is based on this [Paper](https://arxiv.org/abs/2306.02231) and has a squared error loss function based on the estimated advantages (reinforcement learning jargon).\n",
    "- `PromptRewardModel`: Likely computes rewards based on how well an agent responds to prompts or instructions. It could be assessing the relevance according to this [Paper](https://arxiv.org/abs/2303.00001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward weights are then defined in the `text-prompting/prompting/validators/config.py` file which can be set via the command line:\n",
    "\n",
    "```python\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--reward.reciprocate_weight\",\n",
    "        type=float,\n",
    "        help=\"Weight for the reciprocate reward model\",\n",
    "        default=DefaultRewardFrameworkConfig.reciprocate_model_weight,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reward.dpo_weight\",\n",
    "        type=float,\n",
    "        help=\"Weight for the dpo reward model\",\n",
    "        default=DefaultRewardFrameworkConfig.dpo_model_weight,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reward.rlhf_weight\",\n",
    "        type=float,\n",
    "        help=\"Weight for the rlhf reward model\",\n",
    "        default=DefaultRewardFrameworkConfig.rlhf_model_weight,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reward.dahoas_weight\",\n",
    "        type=float,\n",
    "        help=\"Weight for the dahoas reward model\",\n",
    "        default=DefaultRewardFrameworkConfig.dahoas_model_weight,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reward.prompt_based_weight\",\n",
    "        type=float,\n",
    "        help=\"Weight for the prompt-based reward model\",\n",
    "        default=DefaultRewardFrameworkConfig.prompt_model_weight,\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subnet 5: image-generation\n",
    "\n",
    "[Repository](https://github.com/unconst/ImageSubnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Subnet, or text2image, is built to host and run Stable Diffusion models. However, it is adaptable, and any model can be run on the network that takes in a prompt, width, and height paramaters.\n",
    "\n",
    "The Validator is built such that it ranks miners images on aesthetic and how closely they match the given prompt. Also, images which are too similiar in style will be slightly penalized to promote diverisity among image models hosted on miners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rewards are calculated based on three criteria: prompt alignment, dissimilarity, and uniqueness.\n",
    "\n",
    "**Prompt Alignment:**\n",
    "\n",
    "`calculate_rewards_for_prompt_alignment` function (basically `CLIP`):\n",
    "- Processes a query and list of responses, each containing images.\n",
    "- Initializes scores to zero for each response (`init_scores`).\n",
    "- Iterates over each response, processing only those with images.\n",
    "- Generates a score for how well each image aligns with the query's text using a scoring model.\n",
    "- Takes the mean score of all images within a response; if it's negative, it's set to zero.\n",
    "- Performs an exponential function on all scores.\n",
    "- Normalizes the scores by dividing by the sum of all scores, after setting any score that is exactly 1 to 0.\n",
    "\n",
    "**Dissimilarity:**\n",
    "\n",
    "`calculate_dissimilarity_rewards` function:\n",
    "- Takes a list of images and calculates a dissimilarity score for each.\n",
    "- The dissimilarity score is based on how different each image is from the set of images.\n",
    "- If the list of images contains only `None`, zero scores are returned.\n",
    "\n",
    "**Uniqueness (using Perceptual Hash):**\n",
    "\n",
    "The `ImageHashRewards` function calculates hash-based rewards to ensure the uniqueness of images by assigning zero reward to duplicate images:\n",
    "\n",
    "- Initializes a hash map (`hashmap`) for tracking existing image hashes and a list to store the hashes for each image (`hashes`).\n",
    "- Creates a tensor `hash_rewards` with initial values of 1.0 corresponding to the rewards of the same size.\n",
    "- Iterates over all responses, and for responses where the existing reward is zero, sets the corresponding hash reward to zero and appends `None` to the list of hashes for each image in that response.\n",
    "- For valid rewards, it attempts to deserialize the image and computes its perceptual hash (phash) using the `imagehash` library after converting it to a PIL image.\n",
    "- If an exception is raised during deserialization or if the deserialized image is just a black image (sum of pixel values is zero), it logs an error, sets the reward to zero, and appends `None` to the list of hashes for that image.\n",
    "- If the computed hash is already present in the `hashmap`, it also sets the reward to zero for both the current and the previously recorded index to discourage duplicates. \n",
    "- Otherwise, it adds the new hash to the `hashmap` with its corresponding index and appends the hash to the list of hashes.\n",
    "- Returns the updated `hash_rewards`, which now reflect penalties for any duplicate images, and the list of hashes for each response (`hashes`).\n",
    "\n",
    "In the `CalculateRewards` function:\n",
    "- They first normalize the prompt alignment rewards.\n",
    "- Dissimilarity rewards are calculated and normalized separately.\n",
    "- If a reward from prompt alignment is non-zero, the corresponding dissimilarity reward is added to a new tensor, creating a 1:1 relation without losing zero scores.\n",
    "- Rewards then have a dissimilarity weight (15% of total reward) added to encourage diversity among images.\n",
    "- Images are checked for uniqueness using perceptual hash; any duplicates have their reward set to 0.\n",
    "- The rewards are then multiplied by the hash-based rewards.\n",
    "- Finally, the rewards are again normalized so the highest value becomes 1.\n",
    "\n",
    "The function returns:\n",
    "- A tensor of rewards for each response.\n",
    "- A list of perceptual hashes for corresponding images.\n",
    "- The best image selected based on the highest reward.\n",
    "- The hash of the best image.\n",
    "\n",
    "\n",
    "See the following code snippet for an example for more details `ImageSubnet/validator.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "\n",
    "# Determine the rewards based on how close an image aligns to its prompt.\n",
    "def calculate_rewards_for_prompt_alignment(query: TextToImage, responses: List[ TextToImage ]) -> (torch.FloatTensor, List[ Image.Image ]):\n",
    "\n",
    "    # Takes the original query and a list of responses, returns a tensor of rewards equal to the length of the responses.\n",
    "    init_scores = torch.zeros( len(responses), dtype = torch.float32 )\n",
    "    top_images = []\n",
    "\n",
    "    print(\"Calculating rewards for prompt alignment\")\n",
    "    print(f\"Query: {query.text}\")\n",
    "    print(f\"Responses: {len(responses)}\")\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        print(response, type(response))\n",
    "\n",
    "        # if theres no images, skip this response.\n",
    "        if len(response.images) == 0:\n",
    "            top_images.append(None)\n",
    "            continue\n",
    "\n",
    "        img_scores = torch.zeros( num_images, dtype = torch.float32 )\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                images = []\n",
    "\n",
    "                for j, tensor_image in enumerate(response.images):\n",
    "                    # Lets get the image.\n",
    "                    image = transforms.ToPILImage()( bt.Tensor.deserialize(tensor_image) )\n",
    "\n",
    "                    images.append(image)\n",
    "                \n",
    "                ranking, scores = scoring_model.inference_rank(query.text, images)\n",
    "                img_scores = torch.tensor(scores)\n",
    "                # push top image to images (i, image)\n",
    "                if len(images) > 1:\n",
    "                    top_images.append(images[ranking[0]-1])\n",
    "                else:\n",
    "                    top_images.append(images[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"error in \" + str(i))\n",
    "            print(response)\n",
    "            top_images.append(None)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Get the average weight for the uid from _weights.\n",
    "        init_scores[i] = torch.mean( img_scores )\n",
    "        #  if score is < 0, set it to 0\n",
    "        if init_scores[i] < 0:\n",
    "            init_scores[i] = 0\n",
    "        \n",
    "    # if sum is 0 then return empty vector\n",
    "    if torch.sum( init_scores ) == 0:\n",
    "        return (init_scores, top_images)\n",
    "\n",
    "    # preform exp on all values\n",
    "    init_scores = torch.exp( init_scores )\n",
    "\n",
    "    # set all values of 1 to 0\n",
    "    init_scores[init_scores == 1] = 0\n",
    "\n",
    "    # normalize the scores such that they sum to 1 but skip scores that are 0\n",
    "    init_scores = init_scores / torch.sum( init_scores )\n",
    "\n",
    "    return (init_scores, top_images)\n",
    "\n",
    "def calculate_dissimilarity_rewards( images: List[ Image.Image ] ) -> torch.FloatTensor:\n",
    "    # Takes a list of images, returns a tensor of rewards equal to the length of the images.\n",
    "    init_scores = torch.zeros( len(images), dtype = torch.float32 )\n",
    "\n",
    "    # If array is all nones, return 0 vector of length len(images)\n",
    "    if all(image is None for image in images):\n",
    "        return init_scores\n",
    "\n",
    "    # Calculate the dissimilarity matrix.\n",
    "    dissimilarity_matrix = compare_to_set(images)\n",
    "\n",
    "    # Calculate the mean dissimilarity for each image.\n",
    "    mean_dissimilarities = calculate_mean_dissimilarity(dissimilarity_matrix)\n",
    "\n",
    "    # Calculate the rewards.\n",
    "    for i, image in enumerate(images):\n",
    "        init_scores[i] = mean_dissimilarities[i]\n",
    "\n",
    "    return init_scores\n",
    "\n",
    "def CalculateRewards(dendrites_to_query, batch_id, prompt, query, responses, best_image_hash = None):\n",
    "    (rewards, best_images) = calculate_rewards_for_prompt_alignment( query, responses )\n",
    "\n",
    "    if torch.sum( rewards ) == 0:\n",
    "        return rewards, [], None, None\n",
    "    \n",
    "    rewards = rewards / torch.max(rewards)\n",
    "\n",
    "    dissimilarity_rewards: torch.FloatTensor = calculate_dissimilarity_rewards( best_images )\n",
    "\n",
    "    # dissimilarity isnt the same length because we filtered out images with 0 reward, so we need to create a new tensor of length rewards\n",
    "    new_dissimilarity_rewards = torch.zeros( len(rewards), dtype = torch.float32 )\n",
    "\n",
    "    for i, reward in enumerate(rewards):\n",
    "        if reward != 0:\n",
    "            new_dissimilarity_rewards[i] = dissimilarity_rewards[i]\n",
    "\n",
    "    dissimilarity_rewards = new_dissimilarity_rewards\n",
    "\n",
    "    dissimilarity_rewards = dissimilarity_rewards / torch.max(dissimilarity_rewards)\n",
    "\n",
    "    # my goal with dissimilarity_rewards is to encourage diversity in the images\n",
    "\n",
    "    # normalize rewards such that the highest value is 1\n",
    "\n",
    "    dissimilarity_weight = 0.15\n",
    "    rewards = rewards + dissimilarity_weight * dissimilarity_rewards\n",
    "\n",
    "    # Perform imagehash (perceptual hash) on all images. Any matching images are given a reward of 0.\n",
    "    hash_rewards, hashes = ImageHashRewards(dendrites_to_query, responses, rewards)\n",
    "    bt.logging.trace(f\"Hash rewards: {hash_rewards}\")\n",
    "    \n",
    "    # add hashes to the database\n",
    "    for i, _hashes in enumerate(hashes):\n",
    "        try:\n",
    "            resp = responses[i] # TextToImage class\n",
    "            uid = dendrites_to_query[i]\n",
    "            for _hash in _hashes:\n",
    "                hash_already_exists = create_prompt(conn, batch_id, _hash, uid, prompt, \"\", resp.seed, resp.height, resp.width, time.time(), best_image_hash)\n",
    "                if hash_already_exists:\n",
    "                    bt.logging.trace(f\"Detected duplicate image from dendrite {dendrites_to_query[i]}\")\n",
    "                    hash_rewards[i] = 0\n",
    "        except Exception as e:\n",
    "            bt.logging.trace(f\"Error in imagehash: {e}\") if best_image_hash is None else bt.logging.trace(f\"Error in i2i imagehash: {e}\")\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # multiply rewards by hash rewards\n",
    "    rewards = rewards * hash_rewards\n",
    "\n",
    "    if torch.sum( rewards ) == 0:\n",
    "        return rewards, hashes, None, None\n",
    "\n",
    "    # get best image from rewards\n",
    "    best_image_index = torch.argmax(rewards)\n",
    "    best_pil_image = best_images[best_image_index]\n",
    "    if len(hashes[best_image_index]) == 0:\n",
    "        return rewards, hashes, None, None\n",
    "    best_image_hash = hashes[best_image_index][0]\n",
    "    \n",
    "    rewards = rewards / torch.max(rewards)\n",
    "\n",
    "     # log uids\n",
    "    bt.logging.trace(f\"UIDs: {dendrites_to_query}\")\n",
    "    # log all rewards and the best image index / hash\n",
    "    bt.logging.trace(f\"Calculated Rewards: {rewards}\")\n",
    "    # log best score\n",
    "    bt.logging.trace(f\"Best score: {torch.max(rewards)} UID: {dendrites_to_query[best_image_index]} HASH: {best_image_hash}\")\n",
    "\n",
    "    return rewards,hashes,best_pil_image,best_image_hash\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Subnet: Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, one can fork / clone the [Template Repository](https://github.com/opentensor/bittensor-subnet-template) of Bittensor. The template repository contains a basic implementation of a Bittensor subnet, which can be used as a starting point for building your own subnet. \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The Bittensor blockchain hosts multiple self-contained incentive mechanisms called subnets. Subnets are playing fields in which:\n",
    "\n",
    "- Subnet miners who produce value, and\n",
    "- Subnet validators who produce consensus\n",
    "\n",
    "determine together the proper distribution of TAO for the purpose of incentivizing the creation of value, i.e., generating digital commodities, such as intelligence or data.\n",
    "\n",
    "Each subnet consists of:\n",
    "\n",
    "- Subnet miners and subnet validators.\n",
    "- A protocol using which the subnet miners and subnet validators interact with one another. This protocol is part of the incentive mechanism.\n",
    "- The Bittensor API using which the subnet miners and subnet validators interact with Bittensor's onchain consensus engine Yuma Consensus. The Yuma Consensus is designed to drive these actors: subnet validators and subnet miners, into agreement on who is creating value and what that value is worth.\n",
    "\n",
    "This starter template is split into three primary files. To write your own incentive mechanism, you should edit these files. These files are:\n",
    "\n",
    "1. `template/protocol.py`: Contains the definition of the protocol used by subnet miners and subnet validators.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # TODO(developer): Rewrite with your protocol definition.\n",
    "\n",
    "    # This is the protocol for the dummy miner and validator.\n",
    "    # It is a simple request-response protocol where the validator sends a request\n",
    "    # to the miner, and the miner responds with a dummy response.\n",
    "\n",
    "    # ---- miner ----\n",
    "    # Example usage:\n",
    "    #   def dummy( synapse: Dummy ) -> Dummy:\n",
    "    #       synapse.dummy_output = synapse.dummy_input + 1\n",
    "    #       return synapse\n",
    "    #   axon = bt.axon().attach( dummy ).serve(netuid=...).start()\n",
    "\n",
    "    # ---- validator ---\n",
    "    # Example usage:\n",
    "    #   dendrite = bt.dendrite()\n",
    "    #   dummy_output = dendrite.query( Dummy( dummy_input = 1 ) )\n",
    "    #   assert dummy_output == 2\n",
    "\n",
    "\n",
    "    class Dummy(bt.Synapse):\n",
    "        \"\"\"\n",
    "        A simple dummy protocol representation which uses bt.Synapse as its base.\n",
    "        This protocol helps in handling dummy request and response communication between\n",
    "        the miner and the validator.\n",
    "\n",
    "        Attributes:\n",
    "        - dummy_input: An integer value representing the input request sent by the validator.\n",
    "        - dummy_output: An optional integer value which, when filled, represents the response from the miner.\n",
    "        \"\"\"\n",
    "\n",
    "        # Required request input, filled by sending dendrite caller.\n",
    "        dummy_input: int\n",
    "\n",
    "        # Optional request output, filled by recieving axon.\n",
    "        dummy_output: typing.Optional[int] = None\n",
    "\n",
    "        def deserialize(self) -> int:\n",
    "            \"\"\"\n",
    "            Deserialize the dummy output. This method retrieves the response from\n",
    "            the miner in the form of dummy_output, deserializes it and returns it\n",
    "            as the output of the dendrite.query() call.\n",
    "\n",
    "            Returns:\n",
    "            - int: The deserialized response, which in this case is the value of dummy_output.\n",
    "\n",
    "            Example:\n",
    "            Assuming a Dummy instance has a dummy_output value of 5:\n",
    "            >>> dummy_instance = Dummy(dummy_input=4)\n",
    "            >>> dummy_instance.dummy_output = 5\n",
    "            >>> dummy_instance.deserialize()\n",
    "            5\n",
    "            \"\"\"\n",
    "            return self.dummy_output\n",
    "    ...\n",
    "```\n",
    "\n",
    "2. `neurons/miner.py`: Script that defines the subnet miner's behavior, i.e., how the subnet miner responds to requests from subnet validators.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # This is the core miner function, which decides the miner's response to a valid, high-priority request.\n",
    "    def dummy(synapse: template.protocol.Dummy) -> template.protocol.Dummy:\n",
    "        # TODO(developer): Define how miners should process requests.\n",
    "        # This function runs after the synapse has been deserialized (i.e. after synapse.data is available).\n",
    "        # This function runs after the blacklist and priority functions have been called.\n",
    "        # Below: simple template logic: return the input value multiplied by 2.\n",
    "        # If you change this, your miner will lose emission in the network incentive landscape.\n",
    "        synapse.dummy_output = synapse.dummy_input * 2\n",
    "        return synapse\n",
    "    ...\n",
    "```\n",
    "\n",
    "3. `neurons/validator.py`: This script defines the subnet validator's behavior, i.e., how the subnet validator requests information from the subnet miners and determines the scores.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # Step 7: The Main Validation Loop\n",
    "    bt.logging.info(\"Starting validator loop.\")\n",
    "    step = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # TODO(developer): Define how the validator selects a miner to query, how often, etc.\n",
    "            # Broadcast a query to all miners on the network.\n",
    "            responses = dendrite.query(\n",
    "                # Send the query to all miners in the network.\n",
    "                metagraph.axons,\n",
    "                # Construct a dummy query.\n",
    "                template.protocol.Dummy(dummy_input=step),  # Construct a dummy query.\n",
    "                # All responses have the deserialize function called on them before returning.\n",
    "                deserialize=True,\n",
    "            )\n",
    "\n",
    "            # Log the results for monitoring purposes.\n",
    "            bt.logging.info(f\"Received dummy responses: {responses}\")\n",
    "\n",
    "            # TODO(developer): Define how the validator scores responses.\n",
    "            # Adjust the scores based on responses from miners.\n",
    "            for i, resp_i in enumerate(responses):\n",
    "                # Check if the miner has provided the correct response by doubling the dummy input.\n",
    "                # If correct, set their score for this round to 1. Otherwise, set it to 0.\n",
    "                score = template.reward.dummy(step, resp_i)\n",
    "\n",
    "                # Update the global score of the miner.\n",
    "                # This score contributes to the miner's weight in the network.\n",
    "                # A higher weight means that the miner has been consistently responding correctly.\n",
    "                scores[i] = config.alpha * scores[i] + (1 - config.alpha) * score\n",
    "\n",
    "            bt.logging.info(f\"Scores: {scores}\")\n",
    "            # Periodically update the weights on the Bittensor blockchain.\n",
    "            if (step + 1) % 10 == 0:\n",
    "                # TODO(developer): Define how the validator normalizes scores before setting weights.\n",
    "                weights = torch.nn.functional.normalize(scores, p=1.0, dim=0)\n",
    "                bt.logging.info(f\"Setting weights: {weights}\")\n",
    "                # This is a crucial step that updates the incentive mechanism on the Bittensor blockchain.\n",
    "                # Miners with higher scores (or weights) receive a larger share of TAO rewards on this subnet.\n",
    "                result = subtensor.set_weights(\n",
    "                    netuid=config.netuid,  # Subnet to set weights on.\n",
    "                    wallet=wallet,  # Wallet to sign set weights using hotkey.\n",
    "                    uids=metagraph.uids,  # Uids of the miners to set weights for.\n",
    "                    weights=weights,  # Weights to set for the miners.\n",
    "                    wait_for_inclusion=True,\n",
    "                )\n",
    "                if result:\n",
    "                    bt.logging.success(\"Successfully set weights.\")\n",
    "                else:\n",
    "                    bt.logging.error(\"Failed to set weights.\")\n",
    "\n",
    "            # End the current step and prepare for the next iteration.\n",
    "            ...\n",
    "```\n",
    "\n",
    "\n",
    "4. `template/reward.py`: This script defines the subnet reward value for the miner, which is used to update the miner's score.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    def dummy(query: int, response: int) -> float:\n",
    "        \"\"\"\n",
    "        Reward the miner response to the dummy request. This method returns a reward\n",
    "        value for the miner, which is used to update the miner's score.\n",
    "\n",
    "        Returns:\n",
    "        - float: The reward value for the miner.\n",
    "        \"\"\"\n",
    "\n",
    "        return 1.0 if response == query * 2 else 0\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bittensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
